---
title: "Second Hand Car Price Prediction"
author: "Celumusa Fakudze"
date: "2024-05-28"
output: html_document
---

## Data Importing

```{r}
library(readr)
Cars_data <- read_csv("C:/Users/Fakudze/Desktop/Cars data.csv")
View(Cars_data)
```

## Data Examination

```{r}
library(tidyverse)
View(Cars_data)
str(Cars_data)
```


## Variable Selection and data cleaning

```{r}
library(dplyr)
Cars_data <- Cars_data %>% 
           select(-Car_ID) %>% 
           mutate_if(is.character, factor)
```

# **Exploratory Data Analysis**

## Distribution of Dependent variable (Price)

```{r}
library(ggplot2)
library(patchwork)
library(paletteer)

theme_set(theme_light())

hist_plot <- Cars_data %>% 
           ggplot(mapping = aes(Price))+
           geom_histogram(bins = 130, fill = "midnightblue", alpha = 0.9)+
           geom_vline(aes(xintercept = mean(Price), color = "mean"),
                      linetype = "dashed", linewidth = 1.3)+
           geom_vline(aes(xintercept = median(Price), color = "median"),
                      linetype = "dashed", linewidth = 1.3)+
           scale_color_manual(name = "",
                              values = c(mean = "red",
                                         median = "yellow"))

box_plt <- Cars_data %>% 
           ggplot(mapping = aes(Price, 1))+
           geom_boxplot(fill = "green", alpha = 0.7)+
           xlab("Price")+
           ylab("")


(hist_plot/box_plt)+
           plot_annotation(title = "Price Distribution",
                           theme = theme(plot.title = element_text(hjust = 0.5)))


           
```

## Distribution of numeric variables

```{r}
library(summarytools)
numeric_features <- Cars_data %>% 
           select(Price, Power, Kilometers_Driven, Mileage, Engine, Seats)

numeric_features %>% 
           descr(order = "preserve",
                 stats = c("mean", "sd", "min", "q1", "med", "q3", "max"),
                 round.digits = 4)
```

## Pivote to a longer format and Visual numeric features

```{r}
numeric_features <- numeric_features %>% 
           pivot_longer(!Price, names_to = "features", values_to = "values") %>% 
           group_by(features) %>% 
           mutate(mean = mean(values),
                  median = median(values))

numeric_features %>% 
           ggplot(mapping = aes(values, fill = features))+
           geom_histogram(bins = 30, alpha = 0.7, show.legend = F)+
           facet_wrap(~features, scales = "free")+
           geom_vline(aes(xintercept = mean, color = "Mean"),
                      linetype = "dashed", linewidth = 1.3)+
           geom_vline(aes(xintercept = median, color = "Median"),
                      linetype = "dashed", linewidth = 1.3)+
           scale_color_manual(name = "",
                              values = c(Mean = "red",
                                         Median = "yellow"))
```


## Distribution of categorical features

```{r}
categorical_features <- Cars_data %>% 
           select(Owner_Type, Fuel_Type, Transmission, Year, Price) %>% 
           mutate(Year = factor(Year))
```


## Pivote to a longer format and visualize

```{r}
categorical_features <- categorical_features %>% 
           pivot_longer(!Price, names_to = "categories", values_to = "values") %>% 
           group_by(categories) %>% 
           mutate(values = factor(values))

categorical_features %>% 
           ggplot(mapping = aes(values, fill = categories))+
           geom_bar(alpha = 0.7, show.legend = F)+
           geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "white")+
           facet_wrap(~categories, scales = "free")+
           paletteer::scale_fill_paletteer_d("ggthemr::solarized")+
           theme(panel.grid = element_blank(),
                 axis.text.x = element_text(angle = 90))

Cars_data %>% 
           ggplot(aes(Brand, fill = "blue"))+
           geom_bar(alpha = 0.7, show.legend = F)+
           geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "black")
```

## **Checking Relationships**

## Price and numeric features

```{r}
numeric_features %>% 
           mutate(corr_coef = cor(values, Price)) %>% 
           mutate(features = paste(features,'Vs Price, r = ',
                                   corr_coef, sep = '')) %>% 
           ggplot(aes(values, Price, color = features))+
           geom_point(alpha = 0.7, show.legend = F)+
           facet_wrap(~features, scales = "free")+
           scale_x_log10()
```

## Between Price and categorical features

```{r}
categorical_features %>% 
           ggplot(mapping = aes(values, Price, fill = categories))+
           geom_boxplot(alpha = 0.9, show.legend = F)+
           facet_wrap(~categories, scales = "free")+
           theme(panel.grid = element_blank(),
                 axis.text.x = element_text(angle = 90))

Cars_data %>% 
           ggplot(aes(Brand, Price, fill = "blue"))+
           geom_boxplot(alpha = 0.9, show.legend = F)+
           theme(panel.grid = element_blank(),
                 axis.text.x = element_text(angle = 90))

Cars_data %>% 
           ggplot(aes(Model, Price, fill = "blue"))+
           geom_boxplot(alpha = 0.9, show.legend = F)+
           theme(panel.grid = element_blank(),
                 axis.text.x = element_text(angle = 90))
```


## Selecting  sample features with strong relationship with Preice

```{r}
sample <- Cars_data %>% 
           select(c(Price, Engine, Power, Mileage, Year, Transmission, Owner_Type,
                    Fuel_Type, Brand, Model)) %>% 
           mutate(Brand = as.character(Brand)) %>% 
           mutate(Model = as.character(Brand))

str(sample)
```

## Create a data splitting formula

```{r}
library(tidymodels)

set.seed(510)

sample_split <- initial_split(sample,
                              prop = 0.8)


```


## Create a model training and validation sets from sample split

```{r}
train_sample <- training(sample_split)
test_sample <- testing(sample_split)
nrow(train_sample)
nrow(test_sample)
```

## Create a linear model specification

```{r}
lm_spec <- linear_reg() %>% 
           set_engine("lm") %>% 
           set_mode("regression")
```

## Train the linear regression model

```{r}
set.seed(510)
lm_model <- lm_spec %>% 
           fit(Price ~., data = train_sample)
```

## Evaluate the model perfomance by predicting with the test_sample data

```{r}
lm_results <- test_sample %>%
           select(Price) %>% 
           bind_cols(lm_model %>% 
                                predict(new_data = test_sample) %>% 
                                rename(predictions = .pred))

lm_results
```

## Calculate lm_model evaluation metrics

```{r}
eval_metrics <- metric_set(rmse, rsq)

eval_metrics(data = lm_results,
             truth = Price,
             estimate = predictions)
```

## **Can we do better**?

## **Lets fit the model with resapmles**

## Create data preprocessing recipe


```{r}
lm_recipe <- recipe(Price~., data = train_sample) %>% 
           step_dummy(all_nominal_predictors()) %>% 
           step_nzv(all_predictors()) %>% 
           step_normalize(all_numeric_predictors()) %>% 
           step_corr(all_numeric_predictors())
```

## Bundle the lm model specification and the preprocessing recipe into a workflow

```{r}
lm_workflow <- workflow() %>% 
           add_recipe(lm_recipe) %>% 
           add_model(lm_spec)
```

## Create a 10-folds cross validation

```{r}
set.seed(510)
data_folds <- vfold_cv(data = train_sample, v = 10)
```

## Fit the model with resamples

```{r}
set.seed(510)
lm_fit <- fit_resamples(lm_workflow,
                        data_folds,
                        control = control_resamples(save_pred = TRUE))
```

## Evaluate the cross validation model

```{r}
lm_fit %>% 
           collect_predictions()

lm_fit %>% 
           collect_metrics()
```

## **Let's lets look for a better model (fit xgbbost model)**

```{r}
xgboost_spec <- boost_tree(trees = tune(),
                           tree_depth = tune(),
                           learn_rate = tune()) %>% 
           set_engine("xgboost") %>% 
           set_mode("regression")
```

## Data preprocessing recipe for xgboost model

```{r}
xgboost_recipe <- recipe(Price~., data = train_sample) %>% 
           step_dummy(all_nominal_predictors()) %>% 
           step_normalize(all_numeric_predictors())
```

## Bundle the xgboost specification and the recipe into a workflow

```{r}
xgboost_workflow <- workflow() %>% 
           add_recipe(xgboost_recipe) %>% 
           add_model(xgboost_spec)
```

## Create xgboost parameter grid

```{r}
tree_grid <- grid_regular(trees(),
                          tree_depth(),
                          learn_rate(range = c(0.01, 0.3),
                                     trans = NULL), levels = 5)
```

## Tune the xgboost model

```{r}
doParallel::registerDoParallel()

tree_grid <- tune_grid(
           object = xgboost_workflow,
           resamples = data_folds,
           grid = tree_grid
)
```

```{r}
tree_grid %>% 
           show_best("rmse")
```

## Best tree

```{r}
best_tree <- tree_grid %>% 
           select_best("rmse")

best_tree
```

